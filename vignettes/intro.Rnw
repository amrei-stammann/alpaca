\documentclass[10pt]{article}

%\VignetteIndexEntry{Introduction}
%\VignetteEngine{knitr::knitr}

\usepackage[T1]{fontenc}
\usepackage[sfdefault, scaled=.85]{FiraSans}
\usepackage{newtxsf}

\usepackage[english]{babel}
\usepackage{booktabs}
\usepackage[babel]{csquotes}
\usepackage[hmarginratio=1:1, top=32mm, columnsep=20pt]{geometry}
\usepackage{natbib}
\usepackage{url}

\title{Introduction}

\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
require(alpaca)
require(knitr)
opts_chunk$set(size = "small", tidy = TRUE, comment = "")
knit_theme$set("print")
@

\maketitle

\section{Introduction}

The package is well suited to estimate generalized linear models with a linear predictor of the following form:
\begin{equation*}
  \boldsymbol{\eta} = \mathbf{Z} \boldsymbol{\gamma} = \mathbf{D} \boldsymbol{\alpha} + \mathbf{X} \boldsymbol{\beta} = \sum_{k=1}^{K} \mathbf{D}_k \boldsymbol{\alpha}_{k} + \mathbf{X} \boldsymbol{\beta} \, ,
\end{equation*}
where the matrix $\mathbf{D}$ arises from dummy encoding of $K$ high-dimensional categorical variables and $\mathbf{X}$ contains the variables of interst. We refer to $\boldsymbol{\beta}$ as the structural parameters whereas $\boldsymbol{\alpha}$ are the so called fixed effects.

Brute force estimation of this kind of models is often restricted to computational limitations. We tackle this problem by providing a fast and memory efficient algorithm based on the combination of the Frisch-Waugh-Lovell theorem and the method of alternating projections \citep{sta18}. We restrict ourselves to non-linear models since \cite{gau13} already offers a great package for linear models.

\section{Exactness}

To show how to use the package we start with generating an artificial data set of a two-way fixed effects logit model. The data generating process is as follows:
\begin{equation*}
  y_{it} = \mathbf{1}[\mathbf{x}_{it}^{\prime} \boldsymbol{\beta} + \alpha_{i} + \gamma_{t} + \epsilon_{it} > 0] \,,
\end{equation*}
where $\mathbf{x}_{it}$ is generated as iid. $\mathcal{N} \sim (\mathbf{0}_{3}, \mathbf{I}_{3})$ and $\epsilon_{it}$ is an iid. logistic error term with location zero and scale one. $\alpha_{i}$ and $\gamma_{t}$ are generated as iid. standard normal and $\boldsymbol{\beta} = [1, - 1, 1]^{\prime}$. The function \texttt{simGLM(n, t, seed, "logit")} constructs an artificial data set for arbitrary $n$ and $t$.

To compare \texttt{feglm()} to the standard \texttt{glm()} we generate a data set with $n = 200$ and $t = 100$ and extract the estimates of the structural parameters. For \texttt{feglm()} this can be done using the generic function \texttt{coef()}.
<<>>=
# Generate artificial data set
n <- 200L
t <- 100L
data <- simGLM(n, t, 1805L, "logit")

# Use 'feglm()' and 'glm()'
mod.alpaca <- feglm(y ~ x1 + x2 + x3 | i + t, data, family = binomial())
mod.glm <- glm(y ~ x1 + x2 + x3 + factor(i) + factor(t) + 0, data,
               family = binomial())

# Compare estimates of structural parameters
beta.mat <- cbind(coef(mod.alpaca), coef(mod.glm)[seq(3L)])
colnames(beta.mat) <- c("feglm", "glm")
beta.mat

# Show 'summary()'
summary(mod.alpaca)
@

Other generic functions already implemented are: \texttt{fitted()}, \texttt{predict()}, \texttt{summary()}, and \texttt{vcov()}. The standard errors are based on the inverse of the negative Hessian after convergence. The estimates of \texttt{feglm()} and \texttt{glm()} are identical \citep{sta18}.

Now we want to have a look at the estimates of the fixed effects. By default \texttt{glm()} drops the first level of the second category due to perfect collinearity. Thus this level becomes our reference group which means that all remaining coefficients have to be interpreted as difference to the reference group.

The function \texttt{getFEs()} computes the estimated fixed effects from an object returned by \texttt{feglm()}. However the underlying routine does not drop any level while solving the system of equations. Thus an estimable function has to be applied to our solution to get meaningful estimates of the fixed effects. See \cite{gau13} for an extensive treatment of this issue.

In order to reproduce the coefficients of \texttt{glm()} we substract the coefficient of the reference group from all remaining coefficients of the second category and add it to all coefficents of the first category.
<<>>=
# Rank deficient solution
alpha.rd <- getFEs(mod.alpaca)

# Apply estimable function. Use First level of second fixed effects category as reference
alpha <- alpha.rd[- (n + 1L)]
alpha.ref <- alpha.rd[n + 1]
alpha <- c(alpha[seq(n)] + alpha.ref, alpha[seq(n + 1L, n + t - 1L)] - alpha.ref)

# Compare estimates of fixed effects
alpha.mat <- cbind(alpha, coef(mod.glm)[- seq(3L)])
colnames(alpha.mat) <- c("feglm", "glm")
head(alpha.mat)
tail(alpha.mat)
@

\section{Computation Time}

Taking the simulated data set from the previous section and measuring the computation time shows the advantage of \texttt{feglm()} over \texttt{glm()} even in quite small samples.
<<>>=
sec.alpaca <- system.time(mod.alpaca <- feglm(y ~ x1 + x2 + x3 | i + t, data,
                                              family = binomial()))[[3L]]
sec.glm <- system.time(mod.glm <- glm(y ~ x1 + x2 + x3 + factor(i) + factor(t) + 0,
                                      data, family = binomial()))[[3L]]
c(alpaca = sec.alpaca, glm = sec.glm)
@


\bibliographystyle{agsm}
\bibliography{lit}
\end{document}